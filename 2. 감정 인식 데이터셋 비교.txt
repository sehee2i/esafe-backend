2. 감정 인식 데이터셋 비교

본 논문에서는 최근 시선 정보 기반 감정 인식의 필요성과 연구 흐름을 반영하여, 표 1과 같이 시선 정보의 포함 여부에 따라 주요 감정 인식 데이터셋들을 두 그룹으로 나누어 비교하였다. 시선 정보가 포함된 데이터셋은 대부분 단일 모달리티가 아닌 멀티모달(Multimodal) 구성으로 수집된다. 즉, 시선 좌표(eye tracking), 머리 자세(head pose) 등의 시각적 행동 정보 외에도 EEG, ECG, EDA와 같은 생체신호 데이터를 함께 수집하여 보다 풍부한 정서 상태 표현을 가능하게 한다. 이러한 데이터들은 주로 HMD(Head-Mounted Display)나 전용 eye tracker 장비를 활용한 하드웨어 기반 환경에서 수집되며, 고해상도 얼굴 영상과 시선 추적 정보가 정밀하게 동기화되어 있다. 라벨링 방식은 자극 영상에 대한 참가자의 자가 평가(Self-report), 2차원적 평가 척도인 Arousal과 Valence, 또는 다중 감정 범주 분류 방식으로 이루어진다.

반면, 시선 정보가 포함되지 않은 facial 기반 데이터셋은 일반적으로 정적인 이미지 또는 비디오에서 얼굴 표정(Facial Expression)만을 중심으로 감정을 라벨링한다. 이들 데이터셋은 감정 분류(classification) 성능 비교를 위한 벤치마크 용도로 널리 사용되고 있으며, 감정 레이블은 대개 7~8가지의 기본 감정 범주로 제공된다. 특히 최근에는 이러한 facial 기반 데이터셋들이 Vision Transformer(ViT) 기반 모델이나 Mamba 구조 등과 결합되어 Transformer 기반 얼굴 표정 인식 연구에 활발히 활용되는 추세이다.

---

2.1 시선 정보 포함 데이터셋 (Gaze-based)

**2.1-1 MAHNOB-HCI**  
MAHNOB-HCI [5]는 실험 참가자들이 약 40분간의 감정 유발 영상을 36편 시청하는 동안 시선, EEG(32채널), ECG, GSR 등 다양한 생체 및 행동 데이터를 수집한 멀티모달 데이터셋이다. 총 30명의 피험자 중 27명의 유효 데이터가 제공되며, 실험 세션은 15초 프롬포트 영상 및 감정 유발 영상 시청 후 참가자가 5개 항목에 대해 1~9점 자가 평가를 수행하는 방식으로 진행된다. 데이터는 Tobii eye tracker를 통한 시선 좌표, 고정 지점 등을 포함하며, 6대의 카메라(1 컬러, 5 흑백)를 활용한 다각도 얼굴 영상과 환경 비디오도 함께 제공된다. 실험은 시선 보정(calibration) 후 시작되며, 행동 이벤트에 대한 주석 정보도 함께 포함된다.

**2.1-2 SEED-IV**  
SEED-IV [6]는 감정 자극 영상 시청 중 EEG(62채널) 및 시선 데이터를 동시에 수집한 고품질 멀티모달 데이터셋이다. 15명의 참가자가 3일에 걸쳐 총 72개의 영상 클립을 시청하며, 4가지 감정(Happy, Sad, Fearful, Neutral)에 대한 자극에 반응한다. 참가자는 SAM 및 VAS 척도를 통해 자가 평가를 진행하고, 시선 정보는 응시 시간, 도약 지점, 분산 정도 등의 이벤트 기반 통계로 후처리되어 제공된다. EEG 데이터는 5개 주파수 대역의 신호에서 특징을 추출하고 시선 데이터와 연계 분석이 가능하도록 정제된 형태로 구성된다.

---

2.2 시선 정보 미포함 데이터셋 (Facial-based)

**2.2-1 AffectNet**  
AffectNet [7]은 약 100만 장의 정적 얼굴 이미지를 기반으로 구축된 대규모 감정 인식 데이터셋으로, 이 중 약 45만 장에는 8개의 범주형 감정(Happy, Sad 등)과 연속형 감정 차원(Valence, Arousal)이 주석되어 있다. 크라우드소싱 기반 웹 이미지로 수집되어 명시적 참가자 정보는 없으며, 외부 평가자 및 자가 평가 레이블도 함께 제공된다. AffectNet은 이미지 기반 감정 분류 벤치마크로 널리 활용되며, 최근에는 Vision Transformer(ViT), Mamba 등 Transformer 기반 구조의 학습에도 사용되고 있다.

**2.2-2 RECOLA**  
RECOLA [8]는 프랑스어로 진행된 원격 환경에서의 자연스러운 이인 간 대화를 기반으로 수집된 멀티모달 감정 데이터셋이다. 총 23개 팀(46명)이 2인 1조로 구성되어 참여하였으며, 데이터 공유에 동의하지 않은 일부 참가자를 제외하고 27명의 데이터가 공개되어 있다. 시청각 데이터(영상, 음성) 외에도 ECG, EDA 생체 신호가 포함되며, 감정 레이블은 6명의 전문가가 40ms 단위로 연속형 2차원 감정 척도인 Valence 및 Arousal 값을 사용해 주석하였다.
