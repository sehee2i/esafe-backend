ViT) 어딜 보고 집중을 할 지 각 스탭마다 어텐션을 만들어냄 그 단어들이 내가 집중해야 하는 것(어디를 봐야할 지를 알려줌)

1) "show, attention and tell"
* encoder/decoder 항상 생각하기
- 출력으로는 caption으로 나오게됨
- 입력은 이미지가 잘 표현되도록(CNN) -> encoding-> feature v-> rnn 사용-> decoder(현재단어와 현재 피쳐(사진)이 입력으로 들어가면 되는것임)-> 단어나옴

*CNN
-cnn은 몇 개의 층을 쓰더라도 상관없음
- 마지막 cnn 층에 attention을 주면됨
(들어가는 14x14=196)개의 피쳐가 나와야됨(그냥 나의 입력 현재상태로만(여기서는 이때까지 생성된 단어를 이야기 하는것임)
rnn은 앞 단어의 hidden값
가용한 값과 현재 나의 상태(hidden)그래서 총 196개의 숫자를 만들러 가야됨

*RNN
-캡션(단어=나의 상태)는 항상 attention을 할때마다 달라짐

* encoder(CNN파트) 
encoder는 ResNET-101을 사용함(CNN 네트워크)
즉 인코더가 완료된 2048개의 채널값에 "어텐션"을 줄거임(2048,14,14)

*decoder(RNN파트)
- transform (층 맞추기-여기선 linear layer사용함)
ex)2048->200 이런식으로 hidden이 200개이기 때문에 출력값을 이렇게 차원을 맞춰주게됨->그런뒤에 입력으로 디코더에 넣자
->이건 즉 어텐션의 결과임

-> attention 결과값과 h값을 곱한값이 다시 input으로 들어감
->그리고 나서 어텐션 네트워크를 나오게 되면 14X14가 나오게 되어야함


<<내부 설명>>

- merge by python broadcast부분의 2개의 연결 박스가 어텐션 부분에 해당
리니어 레이어(입력,출력값을 넣어줘야됨)-이때 우린 2048,512
BXCXHXW 16X2048X14X14
-> 16x14x14x2048-(linear)>16x14x14x512-(view change)>16x512x14x14

~~> softmax-> 14x14가 나오고 다 attention weight 합 1

----------------------------------------------------------------------------
<<self-attention>>

* 결과물은 쿼리에 갯수만큼 생긴다
-> 쿼리가 여러개면 결과물이 여러개 1000->1000
-> 키 벨류는 항상 고정됨
갯수가 변화는것은 쿼리랑 갯수
"우리가 어텐션을 주고싶은것은" -키와 벨류를 제공하는놈

<트렌스포머>
인코더: 해석 분류, 분해
디코더: 생성

<비전 트렌스포머>
이미지를 파티션으로 나누면 조그마한 이미지가 여러개 나오고
각 조그마한 이미지들을 임베딩함
ex)7x7 컨볼루션 필터 하나 씌우면 결과물 1
20개 씌우면(필터를) 20개 결과물이 나옴
같은 필터가 훅 씌워지면서 각 숫자들을 만들게됨
그리고 2번째 필터가 훅 지나가고 그리고 나서 또 숫자가 나옴

이런식으로
한 이미지 당 1 2 8 ............. 총 10개씩
ㅁㅁㅁㅁㅁㅁㅁㅁㅁㅁ
-> ㅁ 당 밑에
1 2 8 ...... 이렇게..

그게 바로 리니어 프로젝션으로 뭉뚱그려져 되어있음
패치크기의 필터를 적용하여 임베딩 한 것

이 숫자들이 토큰이 되는것이고 
0*
여기서 *이건 논어블 임베딩(nn.parameter)이걸로 적용해서 하면됨--학습을 통해 가중치를,,,,, 그래디언트를 받고난 뒤에 학습을 통해 가중치가 바뀐다는 말임



트랜스포머 인코더안에 selt 어텐션이 들어가져있음
그러면 결과값이 0~9가 나오고 근데 1-9까지는 버려 0만 사용해!
즉 자신 입력에 사용된것만 사용해 (그게 바로 0임)
쿼리는 입력만큼 나오게 되지만 그래도 버려!
(이게 입력을 말하는건가..? 0*, 1() 2() ... 이런것들을?)




트렌스포머의 next 모델로 mamba 예상
어텐션을 쓰지만 오더가 0이다
즉 되게 빠른거임

트렌스포머 모델을 더 공부해보고 싶으면 맘바를 공부해보기





























