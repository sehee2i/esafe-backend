<원래>Gaze-enhanced Crossmodal emotion Embeddings (G-CME)[12]는 음성 데이터와 비디오에서 추출된 얼굴 프레임 정보와 시선 데이터를 입력으로 사용하여 감정인식을 수행하는 멀티모달 모델이다.  Gaze-enhanced Visual Encoder를 통해 시선 기반 특징을 강화하고, Audio Encoder와 함께 생성된 임베딩은 Model-Level Fusion 또는 Early Fusion 방식으로 결합된다. 또한, Crossmodal Triplet Loss를 통해 음성과 시각 임베딩 간 정합성을 높여, 감정 간 유사성과 차이를 효과적으로 반영한다. 최종 분류는 GRU + Linear Classifier를 이용한다.


<수정된거>
Gaze-enhanced Crossmodal emotion Embeddings (G-CME) [12]는 음성 데이터와 비디오에서 추출된 얼굴 프레임 정보와 시선 데이터를 입력으로 사용하여 감정 표현의 정합성과 상호작용을 반영하는 모델을 제안하였다. 먼저, Gaze-enhanced Visual Encoder는 얼굴 이미지에 대해 시선 정보를 결합하여 gaze-aware visual feature를 추출함으로써, 주의를 유도하는 시각적 특징을 강화하고, Audio Encoder는 wav2vec 기반 구조로 음성 특징을 추출하며, 이후 양 모달에서 생성된 임베딩은 Model-level Fusion 또는 Early Fusion 방식으로 결합된다. 또한, Crossmodal Triplet Loss를 통해 음성과 시각 임베딩 사이의 의미적 정합성을 유지하고, 감정 표현 간의 유사성과 차이를 더 명확히 학습할 수 있도록 설계되었다. 최종적으로, 결합된 임베딩은 GRU + Linear Classifier을 거쳐 감정 클래스 (행복, 슬픔 등)로 분류된다.



데이터셋 부분 1시 반까지 작성 후 논문 쉬운거 찾아달라고 지피티한테 부탁하기
내일 9시에 일어나서 계란후라이, 토마토 등 준비한뒤에 토스트 만들기 랩에 싸서 들고가기
내일 학교에 10시 반에 도착이 목표!
